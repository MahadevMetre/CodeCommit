input {
  elasticsearch {
    hosts => ["10.153.10.20:9200"]  # Change to your Elasticsearch host and port
	user => "INGEST_USERNAME"
	password => "INGEST_PASSWORD"
    index => "filebeat*"  # Change to your Elasticsearch index name
    query => '{"query": {"bool": {"must": [],"filter": [{"range": {"@timestamp": {"format": "strict_date_optional_time","gte": "2023-05-17T12:00:00.000Z","lte": "2023-05-17T14:00:00.000Z"}}}]}}}'  # Specify your query
    size => 500  # Number of documents to retrieve per batch
    scroll => "1m"  # Scroll time for each batch
    docinfo => true  # Include document metadata in output
  }
}

output {
  s3 {
    region => "us-east-1"  # Change to your S3 region
    bucket => "elk-backup-bucket-shared"  # Change to your S3 bucket name
    prefix => "es-archive-logs"  # Specify the S3 object prefix
    codec => "json_lines"  # Output format (can be changed as per your requirements)
    temporary_directory => "/etc/logstash/temp"  # Optional: specify a temporary directory for buffering
    size_file => 20480
  }
}
